{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpvO89BvYsSqRV8JFOzmAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkajjy/MyNLPLab/blob/main/N_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOSCOF_hMFXT",
        "outputId": "a843c89f-aeb2-40e0-8455-1c8506b3d70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: emma spared no regrets but he will be always welcome\n",
            "Perplexity: 261.5743700922789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')\n",
        "import numpy as np\n",
        "# 加载语料\n",
        "corpus = nltk.corpus.gutenberg.raw('austen-emma.txt')[:10000]  # 用前 10000 字符\n",
        "tokens = word_tokenize(corpus.lower())\n",
        "\n",
        "# 构建 bigram 模型\n",
        "bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
        "bigram_counts = Counter(bigrams)  # (w_{t-1}, w_t) 的计数\n",
        "unigram_counts = Counter(tokens)  # w_{t-1} 的计数\n",
        "\n",
        "# 计算概率（加一平滑）\n",
        "V = len(unigram_counts)  # 词汇表大小\n",
        "def bigram_prob(w1, w2):\n",
        "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[w1] + V)\n",
        "\n",
        "# 生成文本\n",
        "def generate_text(start_word, max_length=10):\n",
        "    current = start_word\n",
        "    result = [current]\n",
        "    for _ in range(max_length - 1):\n",
        "        # 基于概率采样下一个词\n",
        "        candidates = [(w2, bigram_prob(current, w2)) for (w1, w2) in bigram_counts if w1 == current]\n",
        "        if not candidates:\n",
        "            break\n",
        "        words, probs = zip(*candidates)\n",
        "        current = random.choices(words, weights=probs, k=1)[0]\n",
        "        result.append(current)\n",
        "    return ' '.join(result)\n",
        "\n",
        "# 测试\n",
        "print(\"Generated text:\", generate_text('emma'))\n",
        "\n",
        "# 计算困惑度（简化版）\n",
        "def perplexity(text):\n",
        "    log_prob = 0\n",
        "    n = len(text) - 1\n",
        "    for i in range(n):\n",
        "        w1, w2 = text[i], text[i+1]\n",
        "        prob = bigram_prob(w1, w2)\n",
        "        log_prob += -np.log2(prob) if prob > 0 else float('inf')\n",
        "    return 2 ** (log_prob / n)\n",
        "\n",
        "test_tokens = tokens[:100]\n",
        "print(\"Perplexity:\", perplexity(test_tokens))"
      ]
    }
  ]
}