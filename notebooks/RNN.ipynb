{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO65uoZMeOO8NzDUrk9IujN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkajjy/MyNLPLab/blob/main/notebooks/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lqcAEQl2_ok",
        "outputId": "a31ac838-dbfc-411c-d662-24f3cbad1175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Loss: 0.9218710958957672\n",
            "Epoch 40, Loss: 0.2530360035598278\n",
            "Epoch 60, Loss: 0.0893091419711709\n",
            "Epoch 80, Loss: 0.04507410153746605\n",
            "Epoch 100, Loss: 0.028422897215932608\n",
            "Generated text: the quick brown fox jumps over the lazy dog.\n",
            "the lazy dog.\n",
            "the cat runs fast and jumps high.\n",
            "the cat\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# 准备数据\n",
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "The cat runs fast and jumps high.\n",
        "\"\"\".lower()\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# 将文本转为索引序列\n",
        "sequence = [char_to_idx[c] for c in text]\n",
        "seq_length = 10  # 每次输入的序列长度\n",
        "\n",
        "# 创建训练数据\n",
        "def create_dataset(text, seq_length):\n",
        "    inputs, targets = [], []\n",
        "    for i in range(0, len(text) - seq_length):\n",
        "        inputs.append([char_to_idx[c] for c in text[i:i+seq_length]])\n",
        "        targets.append(char_to_idx[text[i+seq_length]])\n",
        "    return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "inputs, targets = create_dataset(text, seq_length)\n",
        "\n",
        "# 定义 RNN 模型\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.rnn = nn.RNN(hidden_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)  # [batch, seq_length] -> [batch, seq_length, hidden_dim]\n",
        "        out, hidden = self.rnn(x, hidden)  # out: [batch, seq_length, hidden_dim]\n",
        "        out = self.fc(out)  # [batch, seq_length, vocab_size]\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "\n",
        "# 参数\n",
        "hidden_dim = 20\n",
        "n_layers = 1\n",
        "model = CharRNN(vocab_size, hidden_dim, n_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 训练\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    total_loss = 0\n",
        "    for i in range(0, len(inputs), batch_size):\n",
        "        batch_inputs = inputs[i:i+batch_size]\n",
        "        batch_targets = targets[i:i+batch_size]\n",
        "        if len(batch_inputs) == 0:\n",
        "            break\n",
        "        hidden = model.init_hidden(batch_inputs.shape[0])\n",
        "        optimizer.zero_grad()\n",
        "        hidden = hidden.detach()  # 防止梯度累积\n",
        "        output, hidden = model(batch_inputs, hidden)\n",
        "       # loss = criterion(output.view(-1, vocab_size), batch_targets)\n",
        "        loss = criterion(output[:, -1, :].reshape(-1, vocab_size), batch_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / (len(inputs) // batch_size)}\")\n",
        "\n",
        "# 生成文本\n",
        "def generate_text(model, start_text, max_length=100):\n",
        "    model.eval()\n",
        "    chars = [char_to_idx[c] for c in start_text]\n",
        "    hidden = model.init_hidden(1)\n",
        "    result = list(start_text)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length - len(start_text)):\n",
        "            input_tensor = torch.tensor([chars[-seq_length:]], dtype=torch.long)\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            probs = torch.softmax(output[:, -1, :], dim=-1).squeeze()\n",
        "            next_idx = torch.multinomial(probs, 1).item()\n",
        "            result.append(idx_to_char[next_idx])\n",
        "            chars.append(next_idx)\n",
        "    return ''.join(result)\n",
        "\n",
        "# 测试\n",
        "print(\"Generated text:\", generate_text(model, \"the quick\"))\n"
      ]
    }
  ]
}