{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9nvNuoHCWywsnvUiKxMya",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkajjy/MyNLPLab/blob/main/notebooks/skip_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbz-PIsymdHe",
        "outputId": "60647244-6672-41bc-e406-6dbb4552d86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.3863510605121276\n",
            "Epoch 10, Loss: 1.386298258885165\n",
            "Epoch 20, Loss: 1.3862454990873139\n",
            "Epoch 30, Loss: 1.3861922533121782\n",
            "Epoch 40, Loss: 1.3861379891057968\n",
            "Epoch 50, Loss: 1.3860821640555285\n",
            "Epoch 60, Loss: 1.3860242203886375\n",
            "Epoch 70, Loss: 1.3859635794276155\n",
            "Epoch 80, Loss: 1.3858996358482454\n",
            "Epoch 90, Loss: 1.3858317516849183\n",
            "Similarity(cat, dog): 0.8794562745656586\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# 假设词汇表和数据\n",
        "vocab = [\"cat\", \"dog\", \"run\", \"jump\"]  # 简化示例\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "V, d = len(vocab), 2  # 词汇表大小，向量维度\n",
        "\n",
        "# 初始化词向量\n",
        "W = np.random.randn(V, d) * 0.01  # 目标词向量\n",
        "W_context = np.random.randn(V, d) * 0.01  # 上下文向量\n",
        "\n",
        "# 模拟训练数据：(目标词索引, 上下文词索引)\n",
        "training_pairs = [(word_to_idx[\"cat\"], word_to_idx[\"dog\"]),\n",
        "                  (word_to_idx[\"dog\"], word_to_idx[\"run\"])]\n",
        "\n",
        "# 训练 Skip-gram\n",
        "learning_rate = 0.01\n",
        "for epoch in range(100):\n",
        "    loss = 0\n",
        "    for target, context in training_pairs:\n",
        "        # 前向传播\n",
        "        h = W[target]  # 目标词向量\n",
        "        u = np.dot(W_context, h)  # 得分\n",
        "        y_pred = 1 / (1 + np.exp(-u[context]))  # sigmoid\n",
        "\n",
        "        # 损失（简化，只考虑正样本）\n",
        "        loss += -np.log(y_pred)\n",
        "\n",
        "        # 反向传播（更新向量）\n",
        "        grad = y_pred - 1\n",
        "        W_context[context] -= learning_rate * grad * h\n",
        "        W[target] -= learning_rate * grad * W_context[context]\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# 评估：计算词向量相似度\n",
        "def cosine_similarity(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "print(\"Similarity(cat, dog):\", cosine_similarity(W[word_to_idx[\"dog\"]], W[word_to_idx[\"cat\"]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4JB-mSo3nBim"
      }
    }
  ]
}