{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIQN3MuPVUlQsg9YQT4YAA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkajjy/MyNLPLab/blob/main/notebooks/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsURG0nOHmrg",
        "outputId": "09073920-edf3-4cee-ea5e-c2f1e46b37e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Loss: 0.6719115898013115\n",
            "Epoch 40, Loss: 0.11884616315364838\n",
            "Epoch 60, Loss: 0.03753652097657323\n",
            "Epoch 80, Loss: 0.019954499322921038\n",
            "Epoch 100, Loss: 0.012621685164049268\n",
            "Generated text: the quickn jompgs h.ecat uuns fox jumps ovee yatsr and theghegheche cats as tooger.\n",
            "the eayyat ard.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# 准备数据\n",
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "The cat runs fast and jumps high.\n",
        "Cats and dogs play together in the yard.\n",
        "\"\"\".lower()\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# 将文本转为索引序列\n",
        "sequence = [char_to_idx[c] for c in text]\n",
        "seq_length = 10  # 每次输入的序列长度\n",
        "\n",
        "# 创建训练数据\n",
        "def create_dataset(text, seq_length):\n",
        "    inputs, targets = [], []\n",
        "    for i in range(0, len(text) - seq_length):\n",
        "        inputs.append([char_to_idx[c] for c in text[i:i+seq_length]])\n",
        "        targets.append(char_to_idx[text[i+seq_length]])\n",
        "    return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "inputs, targets = create_dataset(text, seq_length)\n",
        "\n",
        "# 定义 LSTM 模型\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, n_layers=1):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)  # [batch, seq_length] -> [batch, seq_length, hidden_dim]\n",
        "        out, hidden = self.lstm(x, hidden)  # out: [batch, seq_length, hidden_dim]\n",
        "        out = self.fc(out)  # [batch, seq_length, vocab_size]\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "# 参数\n",
        "hidden_dim = 20\n",
        "n_layers = 1\n",
        "model = CharLSTM(vocab_size, hidden_dim, n_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    # Initialize hidden state for the entire epoch (adjust if using stateful LSTM across batches)\n",
        "    # Note: For non-stateful LSTM like this, re-initializing per batch is common.\n",
        "    # Let's keep re-initializing per batch as originally coded.\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = 0 # Keep track of number of batches processed\n",
        "\n",
        "    # Create data loader for easier batching and shuffling (optional but good practice)\n",
        "    # from torch.utils.data import TensorDataset, DataLoader\n",
        "    # train_data = TensorDataset(inputs, targets)\n",
        "    # train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "    # for batch_inputs, batch_targets in train_loader: # If using DataLoader\n",
        "\n",
        "    # Original batching loop (works fine too)\n",
        "    hidden = model.init_hidden(batch_size) # Initialize hidden state for the first batch\n",
        "    for i in range(0, len(inputs), batch_size):\n",
        "        end_idx = i + batch_size\n",
        "        # Handle last batch potentially being smaller\n",
        "        actual_batch_size = min(batch_size, len(inputs) - i)\n",
        "        if actual_batch_size == 0:\n",
        "             continue\n",
        "\n",
        "        batch_inputs = inputs[i:end_idx]\n",
        "        batch_targets = targets[i:end_idx]\n",
        "\n",
        "        # Re-initialize hidden state if the batch size changes (important for the last batch)\n",
        "        # Or initialize hidden state here before processing each batch\n",
        "        if i == 0 or actual_batch_size != hidden[0].size(1): # If first batch or size changed\n",
        "            hidden = model.init_hidden(actual_batch_size)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Detach hidden state from the previous batch's history\n",
        "        hidden = tuple(h.detach() for h in hidden)\n",
        "\n",
        "        output, hidden = model(batch_inputs, hidden)\n",
        "\n",
        "        # --- Correction is Here ---\n",
        "        # Select the output corresponding to the last time step for each sequence\n",
        "        last_step_output = output[:, -1, :]\n",
        "        loss = criterion(last_step_output, batch_targets)\n",
        "        # --------------------------\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    if (epoch + 1) % 20 == 0 and num_batches > 0: # Avoid division by zero if dataset is small\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / num_batches}\")\n",
        "    elif num_batches == 0:\n",
        "         print(f\"Epoch {epoch+1}, No batches processed (check dataset size and batch size)\")\n",
        "\n",
        "\n",
        "# --- Small fix in generation for robustness ---\n",
        "def generate_text(model, start_text, max_length=100, seq_length=10): # Pass seq_length\n",
        "    model.eval()\n",
        "    chars_idx = [char_to_idx[c] for c in start_text]\n",
        "    hidden = model.init_hidden(1)\n",
        "    result = list(start_text)\n",
        "\n",
        "    # \"Warm up\" the hidden state with the initial start_text\n",
        "    if len(start_text) > 0:\n",
        "        # Prepare the full start_text as input tensor\n",
        "        input_tensor = torch.tensor([chars_idx], dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "             _, hidden = model(input_tensor, hidden) # Update hidden state based on start_text\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use the last character of start_text as the first input for generation loop\n",
        "        # Or predict based on the warmed-up hidden state\n",
        "        current_char_idx = chars_idx[-1] if chars_idx else char_to_idx.get(' ', 0) # Handle empty start_text\n",
        "\n",
        "        for _ in range(max_length - len(start_text)):\n",
        "            # Input tensor now only needs the *last* character processed\n",
        "            input_tensor = torch.tensor([[current_char_idx]], dtype=torch.long)\n",
        "\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            # Get probabilities for the *single* output step\n",
        "            probs = torch.softmax(output.squeeze(0), dim=-1).squeeze() # Shape [vocab_size]\n",
        "\n",
        "            # Handle potential dimension issues if probs becomes 0-dim\n",
        "            if probs.dim() == 0:\n",
        "                probs = probs.unsqueeze(0)\n",
        "\n",
        "            # Avoid error if probs sums to 0 (unlikely but possible with numerical issues)\n",
        "            if probs.sum() == 0:\n",
        "                 # Fallback: choose a random character or the most frequent one\n",
        "                 next_idx = random.choice(range(vocab_size))\n",
        "                 print(\"Warning: Probability sum is zero, choosing random character.\")\n",
        "            else:\n",
        "                next_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            result.append(idx_to_char[next_idx])\n",
        "            current_char_idx = next_idx # Update the input for the next step\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# Testing (ensure start_text characters are in vocab)\n",
        "valid_start_chars = [c for c in \"the quick\" if c in char_to_idx]\n",
        "if len(valid_start_chars) < len(\"the quick\"):\n",
        "    print(\"Warning: Some characters in start_text are not in the training vocabulary.\")\n",
        "\n",
        "start_text_clean = \"\".join(valid_start_chars)\n",
        "if not start_text_clean: # Handle case where start_text is empty or all invalid chars\n",
        "    start_text_clean = idx_to_char[random.choice(range(vocab_size))] # Start with a random char\n",
        "    print(f\"Warning: Invalid start_text provided, starting with '{start_text_clean}'\")\n",
        "\n",
        "\n",
        "print(\"Generated text:\", generate_text(model, start_text_clean, seq_length=seq_length)) # Pass seq_length"
      ]
    }
  ]
}